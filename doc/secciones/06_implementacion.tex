\chapter{Development follow up}
In this chapter, we will follow development of each milestone. For each one, first, we will describe the issues involved. Then, we will describe the design decisions made, and finally, we will discuss interesting lessons and problems we have encountered in that iteration.

The Github repository used for the development of this project can be found at \href{https://github.com/salvacorts/TFG-Parasitic-Metaheuristics}{github.com/salvacorts/TFG-Parasitic-Metaheuristics}.

\section{Demo Problem}
This milestone contained the following user story:

\subsubsection*{\href{https://github.com/salvacorts/TFG-Parasitic-Metaheuristics/issues/20}{Issue \#20}: As an Administrator, I want to run a demo problem so that I can test the platform} 

Fist of all, we needed a problem that can perform well in the platform. This problem will be used to test the platform as we develop it further.

We chose a multilayer perceptron (MLP) trained with the \textit{Glass} and \textit{Cancer1a} datasets from \textit{proben1} benchmark \ref{proben1}; a set of benchmarks for neural network training algorithms.

The \textit{Glass} dataset is made of 214 samples of glass where each one contains the percent content of 8 different chemical elements, its refractive index, and the type of glass (class attribute). A model that can predict the kind of glass from new samples, can be useful in criminal forensics.

\textit{Cancer1a} is made of 699 samples of breast cancer. Each one contains a sample identification number, 9 attributes about the cell that the sample measures, and the class of cancerous cell: either benign or malignant.

There are two main reasons to have chosen a multilayer perceptron to develop and demo the platform. On the one hand, its fitness function takes more time than the average latency of communication between nodes in the platform. On the other hand, we have previous experience \ref{gprop} with this problem and we are familiar with some useful genetic operators we can apply.

The genetic operators of the genetic algorithm we have used  to optimize the neural network are the same ones proposed by P. A. Castillo et al in G-Prop \cite{gprop}:

\begin{itemize}
	\item \textbf{Selection:} Roulette tournament where two chromosomes are randomly selected from the population to cross.
	
	\item \textbf{Evaluation:} Train a copy of the chro\subsubsection*{\href{https://github.com/salvacorts/TFG-Parasitic-Metaheuristics/issues/20}{Issue \#20}: As an Administrator, I want to run a demo problem so that I can test the platform} mosome with the training dataset without Backpropagation. The fitness of the chromosome that will be returned by this operator will be the classification error obtained by predicting new samples from the validation dataset with the trained copy of the chromosome.
	$$ Error = 1 - \frac{TP + TN}{TP + TN + FP + FN}  $$
	
	Where $TP = $ True Positives, $TN = $ True Negatives, $FP = $ False Positives, and $FN = $ False Negatives.
	
	\item \textbf{Mutation:} First, modify the learning rate of the network by adding a random number uniformly distributed in the range [-0.05, 0.05]. Then, with certain probability, for each neuron in each hidden layer (in our case there is just one hidden layer), add a random number in the range [-0.1, 0.1] following uniform distribution to all its weights.
	
	\item \textbf{Crossover:} Apply a multi-point crossover between two chormosomes where the neurons of both parents will be mixed resulting in two new offspring that will contain neurons from both parents. The learning rates of the two parents are swapped as well.
	
	\item \textbf{Add Neuron:} Add a new randomly initialized neuron to the hidden layer that will be connected with the neurons in the following and previous layer. This operator performs incremental learning and attempts to address the problem of estimating the number of neurons to use in the hidden layer.
	
	\item \textbf{Eliminate Neuron:} Remove a randomly selected neuron from the hidden layer performing decremental learning. This operators aims to reduce the risks of over-fitting due to having too many neurons in the hidden layer.
	
	\item \textbf{Substitute Neuron:} Replace a random neuron from the hidden layer with a new one randomly initialized.
	
	\item \textbf{Train:} This operator performs a local search by training the multi-layer perceptron represented in the chromosome with Backpropagation \cite{backpropagation} for a given number of epochs.
\end{itemize}

We have developed our evolutionary algorithm using the \textit{Go} language \cite{go}. Go, or Golang, is a is a statically typed and compiled open-source programming language that features a C-like syntax but that comes with garbage collection and built-in easy support for concurrent and distributed programming thanks to its \textit{goroutines} \cite{channels} and \textit{channels} \cite{channels}.

There are two main pre-existing libraries that we will use:

\begin{itemize}
	\item \textbf{Go Perceptron:} A single and multilayered perceptron classifier trained with Backpropagation \cite{go-perceptron-go}.
	\item \textbf{eaopt:} An evolutionary optimization library that supports various evolutionary optimization algorithms such as genetic algorithms, particle swarm, and differential evolution among others. It also provides different genetic algorithm models, selection, mutation and crossover operators out of the box \cite{eaopt}.
\end{itemize}

\subsubsection*{Implementation}
As deliverable prototype for the end of this milestone, we wanted to have the same solution running on both native and browser environments. Typically we would have had to write the same code twice; in Go for native and in Java Script for the browser, but thanks to WebAssembly, we could share the same code base between both native and browsers with just minor changes.

The \textit{Go perceptron} library fitted our necessities well so we didn't need to modify it internally at the very first moment. On the other hand, we had to modify \textit{eopt} so we could execute other genetic operators over the population apart from the mutation, evaluation and crossover operators.

In order to use eaopt, first the developer needs to configure the number of generations, the population size and the genetic model to use in the genetic algorithm. The model defines how the algorithm evolves the population with the selection, mutation and crossover operators; in our particular case, we decided to use a generational model.

\begin{lstlisting}[
caption={eaopt's Genome interface},
label={lst:ExtraOperator}, captionpos=b]
type Genome interface {
	Evaluate() (float64, error)
	Mutate(rng *rand.Rand)
	Crossover(genome Genome, rng *rand.Rand)
	Clone() Genome
}
\end{lstlisting}

Since the implementation of these models does not contemplate additional genetic operators apart from those defined in the \textit{Genome} interface from \textit{eaopt}, we had to add an additional member to the generational model class containing an array of extra genetic operators (Listing \ref{lst:ExtraOperator}) that are used after applying mutation and crossover over a given chromosome as seen in Listing \ref{lst:opapp}.

\begin{lstlisting}[
caption={Definition of an extra genetic operator as a struct with a float representing the probability of application, and a function that takes genome and returns a modified one.},
label={lst:ExtraOperator},
captionpos=b]
type ExtraOperator struct {
	Operator    func(Genome, *rand.Rand) Genome
	Probability float64
}
\end{lstlisting}

\begin{lstlisting}[
caption={Application of extra operators after mutating.},
label={lst:opapp},
captionpos=b]
if mod.MutRate > 0 {
	offsprings.Mutate(mod.MutRate, pop.RNG)
}

for i := range offsprings {
	for _, operator := range mod.ExtraOperators {
		if operator.Probability > 0 {
			if pop.RNG.Float64() < operator.Probability {
				offsprings[i].ApplyExtraOperator(operator, pop.RNG)
			}
		}
	}
}
\end{lstlisting} 

As soon as the the genetic operators were implemented and unit tested, we proceeded to fully execute the genetic algorithm.
In order to get some insights about the state of the population during the execution of the genetic algorithm, we logged the fitness and the number of neurons of the best chromosome, and the average fitness of the population after each generation. In Figure \ref{native} we can see how the average and best fitness, as well as the number of neurons of the best solution evolves through the execution of the algorithm.

\begin{lstlisting}[
caption={Some execution logs.},
label={lst:logs},
captionpos=b]
time="2019-07-18T17:51:50+02:00" level=info msg="Best fitness at generation 0: 34.285714" Avg=65.20000000000003 Fitness=34.28571428571429 Generation=0 HiddenLayer_Neurons=16 fields.level=info
time="2019-07-18T17:51:51+02:00" level=info msg="Best fitness at generation 1: 34.285714" Avg=62.45714285714287 Fitness=34.28571428571429 Generation=1 HiddenLayer_Neurons=16 fields.level=info
time="2019-07-18T17:51:52+02:00" level=info msg="Best fitness at generation 2: 31.428571" Avg=62.342857142857135 Fitness=31.42857142857143 Generation=2 HiddenLayer_Neurons=13 fields.level=info
time="2019-07-18T17:51:53+02:00" level=info msg="Best fitness at generation 3: 25.714286" Avg=62.14285714285714 Fitness=25.714285714285708 Generation=3 HiddenLayer_Neurons=19 fields.level=info
\end{lstlisting} 

\begin{figure}[h!]
		\centering
    	\includegraphics[width=\linewidth]{assets/images/milestone1-native-chart.png}
    	\caption{Evolution of the fitness as error predicting the validation dataset and the neural network size}
    	\label{fig:native}
\end{figure} 

We noticed that the execution of the algorithm was taking too long so we used a profiler in order to study which areas of the code were taking most time. It ended up being the \textit{Go perceptron} logs generator. we disabled these logs and the execution time was dramatically reduced from more than 2 hours to around 2 minutes.

Once we were happy about the results, we started to build a simple web interface as a proof of concept for the execution of the algorithm compiled into WebAssembly. We started with a very simple webpage that executed the compiled webassembly and displayed the logs in the console from the developers tools of the browser. But we had two problems with this approach:

On the one hand, when the browser developers tools are open, WebAssembly is actually ran like a JavaScript in order to be able to debug it. So that, the execution time is considerably increased, losing the main advantage of and the reason of using WebAssembly.

As a solution, we added a HTML \textit{Div} to the webpage where we appended the logs from the program execution directly from go using a custom class (listing \ref{lst:weblogger1}) that implemented the \textit{io.Writer} interface. By doing so, we did not need the developer tools anymore, having direct visual feedback from the execution and good performance.

\begin{lstlisting}[
caption={Weblogger class used to append logs to the webpage.},
label={lst:weblogger1},
captionpos=b]
// In main()
log.SetOutput(WebLogger{})
...

type WebLogger struct{}

func (cl WebLogger) Write(p []byte) (o int, err error) {
	logs := dom.Doc.GetElementById("logs")

	new := dom.Doc.CreateElement("p")
	new.SetTextContent(string(p))
	logs.AppendChild(new)

	return 0, nil
}
\end{lstlisting} 

On the other hand, since we were running the webassembly instance directly from a script imported in the HTML page, the same thread that composes the webpage is the one that executes the program and therefore, since this is a computationally intensive tasks, the browser displayed the message from figure \ref{image:kill-task} on top of the webpage. Moreover, if the user changed to another tab, the execution was paused.

\begin{figure}[h!]
		\centering
    	\includegraphics[width=\linewidth]{assets/images/browser-warning.png}
    	\caption{Browser warning about computationally intensive tasks}
    	\label{image:kill-task}
\end{figure} 

We solved this by using a webworker \cite{webworker}; a script that runs in a separate thread in the background. The problem with webworkers is that they do not have access to the DOM, instead, they have to communicate with another thread that is being executed in the browser's main thread. It is a simple but limited API where the main thread launches the worker and sets an event listener for incoming messages. Then the worker posts a new message as a string that is delivered to the main thread. Figure \ref{logging-system} illustrates the webworker behaviour we used for the logging system.

\begin{figure}[h!]
		\centering
    	\includegraphics[width=\linewidth]{assets/images/logging-system.png}
    	\caption{The \textit{main.js} script loaded in the main HTML document, starts a webworker that will start the webassembly runtime and load the compiled Go program into it. The logs generated in Go are received by the main thread through the webworker API and these are appended into the web DOM}
    	\label{image:logging-system}
\end{figure} 

Since now we had a background job doing the most demanding work, the main thread was free to compose the webpage so we added two line charts using \textit{chart JS} to display live information about the execution. In order to parse the information needed to compose the charts from the logs, we changed their format to JSON in order to deserialize them to Java Script objects and access their information faster and easier.

This concludes the first milestone in the project. The deliverable prototype is resulting web interface for the genetic algortihm that trains the multi layer perceptron in WebAssembly. It can be found at \href{https://salvacorts.github.io/TFG-Parasitic-Metaheuristics/mlp-ea/web/src/}{salvacorts.github.io/TFG-Parasitic-Metaheuristics/mlp-ea/web/src/} and looks as follows.

\begin{figure}[h!]
		\centering
    	\includegraphics[width=\linewidth]{assets/images/web-milestone1.png}
    	\caption{Final prototype of the webpage running the Go code for the genetic algorithm in webassembly.}
    	\label{image:web-milestone1}
\end{figure}

The genetic algorithm implementation can be found in the main repository in directory \textit{mlp-ea/common}. The web source can be found in \textit{mlp-ea/web}. The source of the program to execute it natively can be found in \textit{mlp-ea/native} and can be directly run as in Listing \ref{lst:run} or compiled and run as in \ref{lst:build-run}.

\begin{lstlisting}[
caption={Run native program},
label={lst:run},
captionpos=b]
go run main.go
\end{lstlisting} 

\begin{lstlisting}[
caption={Build program and run},
label={lst:build-run},
captionpos=b]
go build main.go
./main
\end{lstlisting} 


\section{Centralized Distributed Evolutionary Algorithm}

In this milestone we aimed to develop a distributed evolutionary algorithm with a client server architecture to solve our MLP problem developed in the previous milestone.

\subsubsection*{\href{https://github.com/salvacorts/TFG-Parasitic-Metaheuristics/issues/9}{Issue \#9}: As a researcher, I want different systems to communicate between each others with a common solution representation so that all of them can work with it and share it.}

Both clients and server need to communicate with each other to work together, to do so, we need to provide a common data representation that enables them to work and exchange chromosomes over the internet. 

We chose Protocol Buffers as our serialization and data definition mechanism since, as we previously saw in chapter 3, it is a simple, fast and extensible solution that outperforms the most commonly used data representation techniques like XML or JSON, and supports many languages which makes it a really attractive alternative for future support for other languages apart from Go.

Moreover, we decided to use \textit{gogoprotobuf} \cite{gogo}, a fork of Protocol Buffers for Golang that comes with important optimization and extra features. Several benchmarks \cite{gogo-bench} comparing this fork with the rest of serialization and deserialization solutions for Golang demonstrates that \textit{gogoprotobuf} is the best choice in terms of performance, correctness, and interoperability.

In Listing \ref{lst:mlp-protobuf} we can find the protocol buffers representation for the Multilayer Perceptron we used in the previous milestone. Once this type is compiled into Golang, its usage slightly differs from a traditional Go type, therefore, we had to adapt all our mlp-related functions from \textit{go-perceptron-go} to work with this new type; including the training and prediction methods, as well as all the genetic operators.

\begin{lstlisting}[
caption={Protocl Buffers representation of the Multilayer Perceptron},
label={lst:mlp-protobuf},
captionpos=b]
enum TransferFunc {
    SIGMOIDAL = 0;
}

message NeuronUnit {
    repeated double Weights = 1;
    double Bias = 2;
    double Lrate = 3;
    double Value = 4;
    double Delta = 5;
}

message NeuralLayer {
    repeated NeuronUnit NeuronUnits = 1 [(gogoproto.nullable) = false];
    int64 Length = 2;
}

message MultiLayerNetwork {
    double LRate = 1;
    repeated NeuralLayer NeuralLayers = 2 [(gogoproto.nullable) = false];
    TransferFunc TFunc = 3;
}
\end{lstlisting} 

In order to test that this new Protocol Buffers based type and the modified functions were behaving as expected, we used the same unit tests we were using for the genetic algorithm using \textit{go-perceptron-go} in the previous iteration to validate our new implementation. We achieved the same performance and results as on the previous milestone.

\subsubsection*{\href{https://github.com/salvacorts/TFG-Parasitic-Metaheuristics/issues/21}{Issue \#21}: As a researcher, I want an interface for browser-based nodes communications so that using the distributed system is easier.}

If two collaborating nodes want to communicate with each other, they will have to enable port-forwarding on their local gateways. Since this project aims to achieve minimal installation requirements, and most routers provided by ISPs does not provide any mechanism to open ports in the router from inside an application (e.g. UPnP \cite{wiki-upnp} is missing or deactivated in most cases) without the explicit intervention of the user for security reasons, we need to rely on a central server that will take the role of intermediaries between clients.

We will base our design on the distributed pool-based genetic algorithm from JJ et al \cite{paper-pool-jj} where the central server maintains a pool of chromosomes that will be modified by the server and evaluated by the clients.

Explain Pool Architecture with (selection?), threads and channels

Explain gRPC vs REST from paper

Explain problems to converge and solutions








































\section{Apendix: Testing and Continuous Integration}